{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Quality Control and Trimming\n",
    "\n",
    "This notebook will go through the workflow for read quality control and trimming. \n",
    "\n",
    "-----------\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Write the run script to check the quality of the reads BEFORE trimming (using fastqc)\n",
    "2. Write the run script to trim and filter low-quality reads with [Trimmomatic](https://carpentries-lab.github.io/metagenomics-analysis/03-trimming-filtering/index.html).\n",
    "3. Write the run script to check the quality of the reads AFTER trimming (using fastqc)\n",
    "4. Launch each of the run scripts using the launcher script.\n",
    "\n",
    "-----------\n",
    "\n",
    "Time to run: Once you launch these scripts, it will take approximately 1-2 hours to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before we get started you will need to set several variables that we will use throughout this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your name and accessions\n",
    "# name is your output directory\n",
    "# accessions is a list of SRA accession ids, or your sample ids.\n",
    "id = \"MY_ID\"\n",
    "accessions = \"MY_ACCESSIONS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/my_dir_path/\" + id + \"/01_qc_trimming\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d323314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fastq directory. This is where we have our raw fastq files.\n",
    "fastq_dir = \"/my_dir_path/\" + id + \"/00_getting_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "Each of the run scripts below executes code that requires certain variables to be set. So we don't need to edit the code in each of the scripts, we are going to use a config file that defines all of these variables. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables. This is generally a good practice in writing scripts on the HPC, that makes it so you only need to modify the config file (rather than each individual run scripts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export ID=$id\" > config.sh\n",
    "!echo \"export ACCESSIONS=$accessions\" >> config.sh\n",
    "!echo \"export FASTQC=/path_to_containers/fastqc-0.11.9.sif\" >> config.sh\n",
    "!echo \"export TRIMMOMATIC=/path_to_containers/trimmomatic:0.39--hdfd78af_2.sif\" >> config.sh\n",
    "!echo \"export WORK_DIR=$work_dir\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=$fastq_dir\" >> config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "### Step 1: Assessing Read Quality for the Raw Reads\n",
    "\n",
    "Now that we have all of our raw sequence data downloaded, we are ready to start the quality control process. We will use a tool called fastqc that generates a report about the quality of our sequence data. First, we create reports showing us the quality of the reads from each accession BEFORE trimming. That way we can see how well our trimming step works.\n",
    "\n",
    "Let's create a run script to run the fastqc program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run fastqc on each of our accessions\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via the `source ./config.sh` command in the script.\n",
    "# 2. fastqc runs on each of the fastq files in the $FASTQ_DIR\n",
    "# 3. We will copy the reports to our home directory to visualize these results (via ondemand Jupyter)\n",
    "# 4. Array is the number of samples, counting from zero\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "# --------------------------------------------------\n",
    "# Request resources here\n",
    "# --------------------------------------------------\n",
    "#SBATCH --job-name=fastqc               # job name\n",
    "#SBATCH --ntasks=1                      # number of CPUs required \n",
    "#SBATCH --cpus-per-task=1               # number of CPU cores per task \n",
    "#SBATCH --nodes=1                       # number of nodes\n",
    "#SBATCH --mem-per-cpu=4000              # memory per CPU core in MB (see also --mem) \n",
    "#SBATCH --time=10:00:00                 # time limit hrs:min:sec\n",
    "#SBATCH --partition=standard            # partition name (i.e. standard, windfall)\n",
    "#SBATCH --account=your_account          # account name (name of your group)                     \n",
    "#SBATCH --output=process-%j.out         # standard output file name (%j expands to jobID)\n",
    "#SBATCH --error=process-%j.err          # standard error file name (%j expands to jobID)                      \n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load modules here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Execute commands here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# echo for log\n",
    "echo \"job started\"; pwd; hostname; date\n",
    "\n",
    "# source config gile and get sample name from input list based on job array index\n",
    "source ./config.sh\n",
    "export SAMPLE=`head -n +${SLURM_ARRAY_TASK_ID} $IN_LIST | tail -n 1`\n",
    "\n",
    "# run FastQC\n",
    "echo \"Running FastQC on sample $SAMPLE\"                       # print sample name to log\n",
    "apptainer run ${FASTQC} fastqc $FASTQ_DIR/${SAMPLE}_*.fastq*\n",
    "\n",
    "# move FastQC results to a separate directory and copy to local directory for viewing\n",
    "# create directory if it does not exist\n",
    "TRIM_DIR=\"${WORK_DIR}/before_qc_trimming\"\n",
    "if [[ ! -d \"$TRIM_DIR\" ]]; then\n",
    "  echo \"$TRIM_DIR does not exist. Directory created\"\n",
    "  mkdir -p $TRIM_DIR\n",
    "fi\n",
    "#  move files\n",
    "mv $FASTQ_DIR/${SAMPLE}_*_fastqc.html $TRIM_DIR\n",
    "mv $FASTQ_DIR/${SAMPLE}_*_fastqc.zip $TRIM_DIR\n",
    "cp -r $TRIM_DIR ~/01_qc_trimming\n",
    "\n",
    "# print date for log\n",
    "echo \"job ended\"; date\n",
    " \n",
    "'''\n",
    "\n",
    "with open('01A_run_fastqc-slurm.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda481",
   "metadata": {},
   "source": [
    "### Step 2: Creating a run script to trim and filter bad reads from the .fastq files\n",
    "\n",
    "In order to run trimmomatic in a PE (paired-end) format we'll need two files. These files are:  *_1.fastq.gz and *_2.fastq.gz for each accession from the SRA (or your sequencing rrun). You downloaded these in 00_getting_data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf10aee",
   "metadata": {},
   "source": [
    "### Initial Data Management\n",
    "\n",
    "Trimmomatic will give us 4 output files (forward paired, forward unpaired, reverse paired and reverse unpaired. To keep our data organized, let's create output directories so the script can organize our data as it runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trimmed and unpaired directories\n",
    "import os\n",
    "\n",
    "trim_dir = work_dir + \"/trimmed_reads\"\n",
    "unpair_dir = work_dir + \"/unpaired_reads\"\n",
    "\n",
    "if os.path.isdir(trim_dir):\n",
    "    print(\"trim_dir exists\")\n",
    "else:\n",
    "    os.mkdir(trim_dir)\n",
    "\n",
    "if os.path.isdir(unpair_dir):\n",
    "    print(\"unpair_dir exists\")\n",
    "else:\n",
    "    os.mkdir(unpair_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e65b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a run script that runs trimmomatic on all of our fastq files\n",
    "# you can only run this after the *.fastq files are gzipped check in your 00_getting_data directory\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "# --------------------------------------------------\n",
    "# Request resources here\n",
    "# --------------------------------------------------\n",
    "#SBATCH --job-name=trimmomatic          # job name\n",
    "#SBATCH --ntasks=1                      # number of CPUs required \n",
    "#SBATCH --cpus-per-task=4               # number of CPU cores per task \n",
    "#SBATCH --nodes=1                       # number of nodes\n",
    "#SBATCH --mem-per-cpu=8000              # memory per CPU core in MB (see also --mem) \n",
    "#SBATCH --time=10:00:00                 # time limit hrs:min:sec\n",
    "#SBATCH --partition=standard            # partition name (i.e. standard, windfall)\n",
    "#SBATCH --account=your_account          # account name (name of your group)                     \n",
    "#SBATCH --output=process-%j.out         # standard output file name (%j expands to jobID)\n",
    "#SBATCH --error=process-%j.err          # standard error file name (%j expands to jobID)            \n",
    " \n",
    "# --------------------------------------------------\n",
    "# Load modules here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Execute commands here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# echo for log\n",
    "echo \"job started\"; pwd; hostname; date\n",
    "\n",
    "# source config gile and get sample name from input list based on job array index\n",
    "source ./config.sh\n",
    "export SAMPLE=`head -n +${SLURM_ARRAY_TASK_ID} $IN_LIST | tail -n 1`\n",
    "\n",
    "# make the output directories \n",
    "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\n",
    "UNPAIR_DIR=\"${WORK_DIR}/unpaired_reads\"\n",
    "\n",
    "echo \"Running Trimmomatic on sample $SAMPLE\"\n",
    "apptainer run ${TRIMMOMATIC} trimmomatic PE -phred33 -threads 4 \\\n",
    "    ${FASTQ_DIR}/${SAMPLE}_R1_001.fastq.gz ${FASTQ_DIR}/${SAMPLE}_R2_001.fastq.gz \\\n",
    "    ${TRIM_DIR}/${SAMPLE}_R1_001.fastq.gz ${UNPAIR_DIR}/${SAMPLE}_R1_001.fastq.gz \\\n",
    "    ${TRIM_DIR}/${SAMPLE}_R2_001.fastq.gz ${UNPAIR_DIR}/${SAMPLE}_R2_001.fastq.gz \\\n",
    "    ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10 SLIDINGWINDOW:4:20 MINLEN:100 HEADCROP:15\n",
    "\n",
    "# print date for log\n",
    "echo \"job ended\"; date\n",
    "'''\n",
    "\n",
    "with open('01B_run_trimmomatic-slurm.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545c522",
   "metadata": {},
   "source": [
    "## Step 3 QC Final Check\n",
    "\n",
    "Create a run script that performs a final quality control check, using fastqc, on the trimmed fastq files.\n",
    "\n",
    "This script will use the fastqc tool, but will check the reads that are in the \"trimmed\" directory. \n",
    "\n",
    "If you have any doubts about the trimming process, you can always run fastqc on the trimmed data and double check that you see all \"green\". You can check the fastqc files using Jupyter to check for any failures or other warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5eb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run fastqc on each of our accessions\n",
    "# Round 2! This will check the fastq files after screening and cleaning with trimmomatic\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "# --------------------------------------------------\n",
    "# Request resources here\n",
    "# --------------------------------------------------\n",
    "#SBATCH --job-name=fastqc_trimmed       # job name\n",
    "#SBATCH --ntasks=1                      # number of CPUs required \n",
    "#SBATCH --cpus-per-task=1               # number of CPU cores per task \n",
    "#SBATCH --nodes=1                       # number of nodes\n",
    "#SBATCH --mem-per-cpu=4000              # memory per CPU core in MB (see also --mem) \n",
    "#SBATCH --time=10:00:00                 # time limit hrs:min:sec\n",
    "#SBATCH --partition=standard            # partition name (i.e. standard, windfall)\n",
    "#SBATCH --account=your_account          # account name (name of your group)                     \n",
    "#SBATCH --output=process-%j.out         # standard output file name (%j expands to jobID)\n",
    "#SBATCH --error=process-%j.err          # standard error file name (%j expands to jobID)                     \n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load modules here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Execute commands here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# echo for log\n",
    "echo \"job started\"; pwd; hostname; date\n",
    "\n",
    "# source config gile and get sample name from input list based on job array index\n",
    "source ./config.sh\n",
    "export SAMPLE=`head -n +${SLURM_ARRAY_TASK_ID} $IN_LIST | tail -n 1`\n",
    "\n",
    "# run FastQC\n",
    "echo \"Running FastQC on sample $SAMPLE\"\n",
    "apptainer run ${FASTQC} fastqc ${WORK_DIR}/trimmed_reads/${SAMPLE}_*.fastq*\n",
    "\n",
    "# move FastQC results to a separate directory and copy to local directory for viewing\n",
    "# create directory if it does not exist\n",
    "TRIM_DIR=\"${WORK_DIR}/after_qc_trimming\"\n",
    "if [[ ! -d \"$TRIM_DIR\" ]]; then\n",
    "  echo \"$TRIM_DIR does not exist. Directory created\"\n",
    "  mkdir -p $TRIM_DIR\n",
    "fi\n",
    "\n",
    "#  move files\n",
    "mv ${WORK_DIR}/trimmed_reads/${SAMPLE}_*_fastqc.html $TRIM_DIR\n",
    "mv ${WORK_DIR}/trimmed_reads/${SAMPLE}_*_fastqc.zip $TRIM_DIR\n",
    "cp -r $TRIM_DIR ~/01_qc_trimming\n",
    "\n",
    "# print date for log\n",
    "echo \"job ended\"; date\n",
    "\n",
    "'''\n",
    "\n",
    "with open('01C_run_fastqc-slurm.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb4e38",
   "metadata": {},
   "source": [
    "## Step 4: Putting it all together\n",
    "\n",
    "Once you have created the the run scripts, you are ready to put them together in a pipeline to run each of the steps one by one. Notice which steps are dependent on the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d650f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the launcher script to kick off our pipeline.\n",
    "\n",
    "my_code = '''#! /bin/bash\n",
    "\n",
    "# 01A_run_fastqc: first job - no dependencies\n",
    "job1=$(sbatch 01A_run_fastqc-slurm.sh)\n",
    "jid1=$(echo $job1 | sed 's/^Submitted batch job //')\n",
    "echo $jid1\n",
    "\n",
    "# 01B_run_trimmomatic: jid2 depends on jid1\n",
    "job2=$(sbatch --dependency=afterok:$jid1 01B_run_trimmomatic-slurm.sh)\n",
    "jid2=$(echo $job2 | sed 's/^Submitted batch job //')\n",
    "echo $jid2\n",
    "\n",
    "# 01C_run_fastqc: jid3 depends on jid2\n",
    "job3=$(sbatch --dependency=afterok:$jid2 01C_run_fastqc-slurm.sh)\n",
    "jid3=$(echo $job3 | sed 's/^Submitted batch job //')\n",
    "echo $jid3\n",
    "\n",
    "'''\n",
    "\n",
    "with open('01_launch_pipeline-slurm.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066ee552",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x *sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e001991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it!\n",
    "!./01_launch_pipeline-slurm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# Check for all jobs under your netid\n",
    "# Notice that 06B jobs are dependent on 06A jobs finishing and etc.\n",
    "!squeue --user=$id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1e582",
   "metadata": {},
   "source": [
    "### What happens next?\n",
    "\n",
    "Your code will take a little time to get \"picked up\" by the HPC and move from PD (pending) to R (running). Come back shortly to check on it. But, for now, relax and enjoy your day!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "### Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f247f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
