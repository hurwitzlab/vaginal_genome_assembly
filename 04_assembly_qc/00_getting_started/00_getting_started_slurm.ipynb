{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8de6a9",
   "metadata": {},
   "source": [
    "# 00: Getting started (SLURM)\n",
    "\n",
    "## Job Arrays\n",
    "All the notebooks in this repository show you how to run the analysis using **job arrays**. Job arrays are a way to run the same job multiple times with different input files. This is useful for running the same analysis on multiple samples, as is often the case in bioinformatics. \n",
    " \n",
    "Our basic **job array submission structure** requires the following files: \n",
    "- A text file (e.g. `sample_list.txt`) that contains a list of input files, one per line. Each line corresponds to a different sample.\n",
    "- A job script (e.g. `process.slurm`) that contains the commands to be run. This script will be executed multiple times, once for each sample. Note that **\"process\"** can be any analysis you want to run (e.g. `fastqc`, `multiqc`, `kraken2`, etc.).\n",
    "- A configuration file  (e.g. `config.sh`) that contains parameters, variables, and settings for the job script.\n",
    "- A job submission script, or launcher script, (e.g. `run_process.sh`) that submits the job array to the scheduler. It specifies the range of job indices to be run, which corresponds to the number of lines in the text file.\n",
    "\n",
    "\n",
    "## Scheduler\n",
    "A scheduler is a software that manages the resources of a computer cluster and allocates them to users. It is responsible for scheduling jobs, managing queues, and monitoring the status of jobs.\n",
    "\n",
    "> Note that all the notebooks in this repository assume you are using a **SLURM** (`fastqc.slurm`) or **LSF** (`fastqc.lsf`)  scheduler. If you are using a different scheduler, you will need to modify the job scripts and submission scripts accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9613297",
   "metadata": {},
   "source": [
    "## Job script\n",
    "The job script is a bash script that contains the commands to be run. It is executed by the scheduler. Here we provide a template job script that you can modify for your own analysis. \n",
    "\n",
    "A few important points:\n",
    "1. We are using the variables from the config file via the `source ./config.sh` command in the script.\n",
    "2. Our process runs on each of the fastq files in the $FASTQ_DIR\n",
    "3. We will copy the reports to our home directory to visualize these results (via ondemand Jupyter)\n",
    "4. Array is the number of samples, counting from zero   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86d1fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5865d6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a template job script for LSF scheduler\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "# --------------------------------------------------\n",
    "# Request resources here\n",
    "# --------------------------------------------------\n",
    "#SBATCH --job-name=process              # job name\n",
    "#SBATCH --ntasks=1                      # number of CPUs required \n",
    "#SBATCH --cpus-per-task=1               # number of CPU cores per task \n",
    "#SBATCH --nodes=1                       # number of nodes\n",
    "#SBATCH --mem-per-cpu=4000              # memory per CPU core in MB (see also --mem) \n",
    "#SBATCH --time=10:00:00                 # time limit hrs:min:sec\n",
    "#SBATCH --partition=standard            # partition name (i.e. standard, windfall)\n",
    "#SBATCH --account=your_account          # account name (name of your group)                     \n",
    "#SBATCH --output=process-%j.out         # standard output file name (%j expands to jobID)\n",
    "#SBATCH --error=process-%j.err          # standard error file name (%j expands to jobID)\n",
    "                        \n",
    "# --------------------------------------------------\n",
    "# Load modules here\n",
    "# --------------------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Execute commands here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# echo for log\n",
    "echo \"job started\"; pwd; hostname; date\n",
    "\n",
    "# source the config file\n",
    "source ./config.sh\n",
    "\n",
    "# get sample ID\n",
    "export SAMPLE=`head -n +${SLURM_ARRAY_TASK_ID} $IN_LIST | tail -n 1`\n",
    "\n",
    "\n",
    "# echo for log\n",
    "echo \"job done\"; date\n",
    " \n",
    "'''\n",
    "\n",
    "with open('template.slurm', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3e2fa",
   "metadata": {},
   "source": [
    "## Config file\n",
    "All the jobs scripts in this repository use a configuration file (`config.sh`) to store parameters, variables, and settings. This file is sourced in the job scripts to make the variables available. Here we provide a template config file that you can modify for your own analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd2a8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f2f2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a template config file\n",
    "\n",
    "my_code = '''#!/bin/bash\n",
    "IN_LIST=/path_to_my_file/sample_list.txt\n",
    "FASTQC=/path_to_containers/container\n",
    "WORK_DIR=/my_dir_path/MY_ID/\n",
    "\n",
    "'''\n",
    "\n",
    "with open('template_config.sh', mode='w') as file:\n",
    "    file.write(my_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd843e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Launcher script\n",
    "The launcher script is a bash script that submits the job array to the scheduler. It specifies the range of job indices to be run, which corresponds to the number of lines in the text file. Here we provide a template launcher script that you can modify for your own analysis.\n",
    "\n",
    "##### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb6789",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "my_code = '''#!/bin/bash -l\n",
    "\n",
    "# load job configuration\n",
    "source ./config.sh\n",
    "\n",
    "#\n",
    "# make sure sample file is in the right place\n",
    "#\n",
    "if [[ ! -f \"$IN_LIST\" ]]; then\n",
    "    echo \"$IN_LIST does not exist. Please provide the path for a list of datasets to process. Job terminated.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# get number of samples to process\n",
    "# the number of samples will be used to set the range of the job array\n",
    "export NUM_JOB=$(wc -l < \"$IN_LIST\")\n",
    "\n",
    "# submit job array\n",
    "echo \"launching process.slurm as a job.\"\n",
    "\n",
    "JOB_ID=`sbatch --job-name process -a 1-$NUM_JOB process.slurm`\n",
    "\n",
    "'''\n",
    "\n",
    "with open('run_process.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d188bf9",
   "metadata": {},
   "source": [
    "> **Remember!** that in all these templates you will need to modify the word **\"process\"** to the name of the process you are running (e.g. `fastqc`, `multiqc`, `kraken2`, etc.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
